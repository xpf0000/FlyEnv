{
  "OLLAMA_DEBUG": "Afficher des informations de débogage supplémentaires (par exemple OLLAMA_DEBUG=1)",
  "OLLAMA_HOST": "Adresse IP pour le serveur ollama (par défaut 127.0.0.1:11434)",
  "OLLAMA_KEEP_ALIVE": "La durée pendant laquelle les modèles restent chargés en mémoire (par défaut \"5m\")",
  "OLLAMA_MAX_LOADED_MODELS": "Nombre maximum de modèles chargés par GPU",
  "OLLAMA_MAX_QUEUE": "Nombre maximum de requêtes en file d'attente",
  "OLLAMA_MODELS": "Le chemin vers le répertoire des modèles",
  "OLLAMA_NUM_PARALLEL": "Nombre maximum de requêtes parallèles",
  "OLLAMA_NOPRUNE": "Ne pas purger les blobs de modèles au démarrage",
  "OLLAMA_ORIGINS": "Une liste séparée par des virgules des origines autorisées",
  "OLLAMA_SCHED_SPREAD": "Toujours planifier le modèle sur tous les GPU",
  "OLLAMA_FLASH_ATTENTION": "Activer l'attention flash",
  "OLLAMA_KV_CACHE_TYPE": "Type de quantification pour le cache K/V (par défaut : f16)",
  "OLLAMA_LLM_LIBRARY": "Définir la bibliothèque LLM pour contourner la détection automatique",
  "OLLAMA_GPU_OVERHEAD": "Réserver une partie de VRAM par GPU (octets)",
  "OLLAMA_LOAD_TIMEOUT": "Combien de temps faut-il pour laisser les charges du modèle stagner avant d'abandonner ? (par défaut \"5m\")",
  "needServiceRun": "Veuillez d'abord démarrer le service Olama",
  "size": "Taille",
  "model": "Modèles"
}
